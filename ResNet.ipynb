{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0ac129-4b37-4804-8613-3b639d238c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda update -n base conda\n",
    "# %conda uninstall pytorch\n",
    "# %conda install pytorch\n",
    "# %conda install torchvision\n",
    "# %pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fe482f3-2fa2-47d3-ab9b-0e1616293a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchsummary\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551c0cf9-162e-42d1-a813-e58d4871c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "# _, term_width = os.popen('stty size', 'r').read().split()\n",
    "term_width = 50\n",
    "\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8107b7-5d71-4168-88ad-ca44e8e1ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2688f85b-1fc7-48a0-b1f6-0373074e3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb8fbf6-ee1d-4fd3-bfc1-62dd014c5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50d30e2-ae7f-44ed-a9ef-8b6362b4066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ab99e-ef76-4a7e-b42f-645c29cd6b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "The size of the model is: 42.597MB\n",
      "\n",
      "Epoch: 0\n",
      "Training epoch 0: Loss=1.493 Acc=45.820\n",
      "Testing epoch 0: Loss=1.471 Acc=52.640\n",
      "Saving..\n",
      "Epoch 0: 40.96s\n",
      "Train: 38.21s, Test: 2.76s\n",
      "\n",
      "Epoch: 1\n",
      "Training epoch 1: Loss=0.950 Acc=66.394\n",
      "Testing epoch 1: Loss=0.913 Acc=69.730\n",
      "Saving..\n",
      "Epoch 1: 78.44s\n",
      "Train: 34.99s, Test: 2.50s\n",
      "\n",
      "Epoch: 2\n",
      "Training epoch 2: Loss=0.726 Acc=74.764\n",
      "Testing epoch 2: Loss=0.833 Acc=73.550\n",
      "Saving..\n",
      "Epoch 2: 116.00s\n",
      "Train: 35.01s, Test: 2.54s\n",
      "\n",
      "Epoch: 3\n",
      "Training epoch 3: Loss=0.611 Acc=78.774\n",
      "Testing epoch 3: Loss=0.699 Acc=76.490\n",
      "Saving..\n",
      "Epoch 3: 153.68s\n",
      "Train: 35.13s, Test: 2.56s\n",
      "\n",
      "Epoch: 4\n",
      "Training epoch 4: Loss=0.529 Acc=81.816\n",
      "Testing epoch 4: Loss=0.632 Acc=79.140\n",
      "Saving..\n",
      "Epoch 4: 191.48s\n",
      "Train: 35.23s, Test: 2.57s\n",
      "\n",
      "Epoch: 5\n",
      "Training epoch 5: Loss=0.473 Acc=83.556\n",
      "Testing epoch 5: Loss=0.588 Acc=81.210\n",
      "Saving..\n",
      "Epoch 5: 229.56s\n",
      "Train: 35.47s, Test: 2.62s\n",
      "\n",
      "Epoch: 6\n",
      "Training epoch 6: Loss=0.428 Acc=85.114\n",
      "Testing epoch 6: Loss=0.501 Acc=83.740\n",
      "Saving..\n",
      "Epoch 6: 268.06s\n",
      "Train: 35.88s, Test: 2.62s\n",
      "\n",
      "Epoch: 7\n",
      "Training epoch 7: Loss=0.389 Acc=86.562\n",
      "Testing epoch 7: Loss=0.480 Acc=84.390\n",
      "Saving..\n",
      "Epoch 7: 307.23s\n",
      "Train: 36.52s, Test: 2.65s\n",
      "\n",
      "Epoch: 8\n",
      "Training epoch 8: Loss=0.360 Acc=87.564\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "# parser.add_argument('--resume', '-r', action='store_true',\n",
    "#                     help='resume from checkpoint')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "mode = 'single'  # 'amp', 'half' or 'single'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "Learning_rate = 0.0001\n",
    "Resume = False\n",
    "Optimizer = 'sgd'  # sgd, sgdn, adagrad, adadelta or adam\n",
    "Batch_size = 128\n",
    "\n",
    "half = False\n",
    "amp = False\n",
    "if mode == 'half':\n",
    "    half = True\n",
    "elif mode == 'amp':\n",
    "    amp = True\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=Batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if Resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "\n",
    "if half:\n",
    "    net = net.half()\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.BatchNorm2d):\n",
    "            layer.float()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.SGD(net.parameters(), lr=Learning_rate,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "if Optimizer == 'sgdn':\n",
    "    optimizer = optim.SGD(net.parameters(), lr=Learning_rate,\n",
    "                          momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "elif Optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(net.parameters(), lr=Learning_rate,\n",
    "                              weight_decay=5e-4)\n",
    "elif Optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(net.parameters(), lr=Learning_rate,\n",
    "                               weight_decay=5e-4)\n",
    "elif Optimizer == 'adam':\n",
    "    optimizer = optim.Adam(net.parameters(), lr=Learning_rate,\n",
    "                           weight_decay=5e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# torchsummary.summary(net, (3, 32, 32))\n",
    "\n",
    "\n",
    "def getModelSize(model):\n",
    "    param_size = 0\n",
    "    param_sum = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "        param_sum += param.nelement()\n",
    "    buffer_size = 0\n",
    "    buffer_sum = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "        buffer_sum += buffer.nelement()\n",
    "    all_size = (param_size + buffer_size) / 1024 / 1024\n",
    "    print('The size of the model is: {:.3f}MB'.format(all_size))\n",
    "    return (param_size, param_sum, buffer_size, buffer_sum, all_size)\n",
    "\n",
    "\n",
    "getModelSize(net)\n",
    "\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_training_time = 0\n",
    "    time1 = torch.cuda.Event(enable_timing=True)\n",
    "    time2 = torch.cuda.Event(enable_timing=True)\n",
    "    time1.record()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if half:\n",
    "            inputs = inputs.half()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        time2.record()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        total_training_time += time1.elapsed_time(time2)\n",
    "        \n",
    "    avg_loss = train_loss/total\n",
    "    acc = 100.*correct/total\n",
    "    print('Training epoch %d: Loss=%.3f Acc=%.3f' %\n",
    "          (epoch, avg_loss, acc))\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if half:\n",
    "                inputs = inputs.half()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    avg_loss = test_loss/total\n",
    "    acc = 100.*correct/total\n",
    "    print('Testing epoch %d: Loss=%.3f Acc=%.3f' %\n",
    "          (epoch, avg_loss, acc))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "        \n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "filename = mode + '.csv'\n",
    "file = open(filename, 'w')\n",
    "file.write('train_avg_loss, train_acc, test_avg_loss, test_acc, train_time, test_time\\n')\n",
    "file.close()\n",
    "Start_time = time.perf_counter()\n",
    "for epoch in range(start_epoch, start_epoch+150):\n",
    "    time1 = time.perf_counter()\n",
    "    train_avg_loss, train_acc = train(epoch)\n",
    "    time2 = time.perf_counter()\n",
    "    test_avg_loss, test_acc = test(epoch)\n",
    "    Cur_time = time.perf_counter()\n",
    "    # scheduler.step()\n",
    "    print('Epoch %d: %.2fs' % (epoch, Cur_time - Start_time))\n",
    "    print('Train: %.2fs, Test: %.2fs' % (time2 - time1, Cur_time - time2))\n",
    "    file = open(filename, 'a')\n",
    "    file.write('%.3f, %.3f, %.3f, %.3f, %.2f, %.2f\\n' %\n",
    "               (train_avg_loss, train_acc, test_avg_loss, test_acc,\n",
    "                time2 - time1, Cur_time - time2))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341efb2-ddf2-4528-a047-7c2569968626",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
